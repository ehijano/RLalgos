{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "Some background: https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "Nice explanation: https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665\n",
    "Nice video: https://www.youtube.com/watch?v=LN29DDlHp1U&ab_channel=YannBouteiller\n",
    "\n",
    "off-policy:\n",
    "\n",
    "actor-critic:\n",
    "\n",
    "encourages exploration - maximizes entropy: No maximize reward of action, but maximize entropy to randomize actions and encourage exploration\n",
    "\n",
    "\n",
    "\n",
    "Issues:\n",
    "- Not a lot of exploration - Local optima are an issue\n",
    "- Unseen scenarios yield poor performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SAC logic](img/SAC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams\n",
    "\n",
    "\n",
    "### ALPHA \n",
    "Tradeoff between exploration and exploitation. \n",
    "\n",
    "Balances the return vs the entropy of the policy\n",
    "\n",
    "Participates in\n",
    "- Actor loss: \n",
    "$$  \\text{Loss} = - \\alpha \\times \\text{entropy} - Q $$\n",
    "with $\\text{entropy} = -\\log p$. This way we optimize both the entropy (more exploration), and the Q value (reward). \n",
    "- reparametrization trick temperature parameter\n",
    "\n",
    "It is learned iteratively - We only control the initial value. Learning is done by trying to set the log proba of actions to a target entropy value\n",
    "\n",
    "\n",
    "### GAMMA\n",
    "Discount factor, for relative importance between current (0) and future (1) rewards. The relevant formula is\n",
    "\n",
    "$$ \\text{target } Q (s_t, a_t)= r(s_t, a_t) +  \\gamma \\times (1-\\text{done}) \\times \\left( \\text{min } Q_{\\text{critics}} + \\alpha \\times \\text{entropy}   \\right) $$\n",
    "with $r = \\text{reward from replay buffer}$, \n",
    "\n",
    "\n",
    "### TAU\n",
    "Polyak coefficient. Determines how much the target networks update. In this case there is target actor and two target critics.\n",
    "\n",
    "$$ \\text{target} = \\tau \\times \\text{data} + (1 - \\tau) \\times \\text{target} $$\n",
    "\n",
    "- TAU = 0 means targets do not update\n",
    "- TAU = 1 means targets update exactly. Better for convergence, but introduces instabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "ALPHA = 0.2\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 1e6\n",
    "LEARNING_RATE = 3e-4\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "# Create an environment\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "# Define the actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 256)\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.layer_3 = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_min = -20\n",
    "        self.log_std_max = 2\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.layer_1(state))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        mu = self.layer_3(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal_distribution = Normal(mu, std)\n",
    "        z = normal_distribution.rsample()\n",
    "        action = torch.tanh(z) * self.max_action\n",
    "        log_prob = normal_distribution.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.layer_3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = torch.relu(self.layer_1(torch.cat([x, u], 1)))\n",
    "        x = torch.relu(self.layer_2(x))\n",
    "        x = self.layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "\n",
    "Stores experiences experienced by the agent in the form of a tuple  (state, action, next_state, reward, done)\n",
    "\n",
    "Used during training to break the correlation between consecutive samples.\n",
    "\n",
    "Allows efficient reuse of experiences in training\n",
    "\n",
    "Allows to go off-policy, as the experiences recorded do not correspond to the current policy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        batch_states, batch_actions, batch_next_states, batch_rewards, batch_dones = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            state, action, next_state, reward, done = self.storage[i]\n",
    "            batch_states.append(state)\n",
    "            batch_actions.append(action)\n",
    "            batch_next_states.append(next_state)\n",
    "            batch_rewards.append(reward)\n",
    "            batch_dones.append(done)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(batch_states),\n",
    "            torch.FloatTensor(batch_actions),\n",
    "            torch.FloatTensor(batch_next_states),\n",
    "            torch.FloatTensor(batch_rewards),\n",
    "            torch.FloatTensor(batch_dones),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, env):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.max_action = float(env.action_space.high[0])\n",
    "\n",
    "        # Actor object\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.max_action)\n",
    "\n",
    "        # Target is an actor that is updated less often than the Actor - Initially it must be initialized as a copy of actor. \n",
    "        self.actor_target = Actor(self.state_dim, self.action_dim, self.max_action)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # Optimized\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Critics\n",
    "        self.critic_1 = Critic(self.state_dim, self.action_dim)\n",
    "        self.critic_2 = Critic(self.state_dim, self.action_dim)\n",
    "        self.critic_target_1 = Critic(self.state_dim, self.action_dim)\n",
    "        self.critic_target_2 = Critic(self.state_dim, self.action_dim)\n",
    "        self.critic_target_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_target_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.critic_optimizer_1 = optim.Adam(self.critic_1.parameters(), lr=LEARNING_RATE)\n",
    "        self.critic_optimizer_2 = optim.Adam(self.critic_2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.log(ALPHA), requires_grad=True)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=LEARNING_RATE)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "    def train(self, iterations):\n",
    "        # Main training loop\n",
    "        for _ in range(iterations):\n",
    "\n",
    "            # Sample a batch of experiences from the replay buffer\n",
    "            state, action, next_state, reward, done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Compute the TARGET Q value (expected reward)\n",
    "            # It is the minimum reward computed from the critic response to a sampled action from the actor target\n",
    "            with torch.no_grad():\n",
    "                target_action, log_prob = self.actor_target.sample(next_state)\n",
    "                target_q1 = self.critic_target_1(next_state, target_action)\n",
    "                target_q2 = self.critic_target_2(next_state, target_action)\n",
    "                # Alpha here encourages exploration by introducing an entropy term, which is the log of the prob of the sampled action\n",
    "                target_q = torch.min(target_q1, target_q2) - self.alpha * log_prob\n",
    "                target_q = reward + (1 - done) * GAMMA * target_q\n",
    "\n",
    "            # Update the critics\n",
    "            # Critics are networks who should estimate target Q correctly!\n",
    "            current_q1 = self.critic_1(state, action)\n",
    "            current_q2 = self.critic_2(state, action)\n",
    "            critic_loss_1 = nn.MSELoss()(current_q1, target_q)\n",
    "            critic_loss_2 = nn.MSELoss()(current_q2, target_q)\n",
    "            self.critic_optimizer_1.zero_grad()\n",
    "            critic_loss_1.backward()\n",
    "            self.critic_optimizer_1.step()\n",
    "            self.critic_optimizer_2.zero_grad()\n",
    "            critic_loss_2.backward()\n",
    "            self.critic_optimizer_2.step()\n",
    "\n",
    "            # Update the actor and alpha\n",
    "            # Actor update seeks to improve the returns (regularized by the entropy term)\n",
    "            new_action, log_prob = self.actor.sample(state)\n",
    "            q1_new = self.critic_1(state, new_action)\n",
    "            q2_new = self.critic_2(state, new_action)\n",
    "            q_new = torch.min(q1_new, q2_new)  # q-value determined by the critics\n",
    "            # Actor must optimize the q-value, but there is an entropy term to encourage exploration\n",
    "            actor_loss = (self.alpha * log_prob - q_new).mean() \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Alpha is trained to be the parameter that makes the entropy (log_prob) equal to the target entropy (hyperparam)\n",
    "            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            \n",
    "            # Soft update on target parameters\n",
    "            for param, target_param in zip(self.critic_1.parameters(), self.critic_target_1.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.critic_2.parameters(), self.critic_target_2.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "    def run(self):\n",
    "        for episode in range(EPISODES):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            while True:\n",
    "                action = self.actor.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add((state, action, next_state, reward, float(done)))\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if len(self.replay_buffer.storage) >= BATCH_SIZE:\n",
    "                    self.train(1)\n",
    "\n",
    "                if done:\n",
    "                    print(f\"Episode: {episode + 1}, Reward: {episode_reward}\")\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
